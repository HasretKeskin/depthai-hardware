.. _sync_frames:

Synchronizing frames
====================

There are 2 way to synchronize messages from different sensors (frames, IMU packet, ToF, etc.);

- :ref:`Software syncing <Software "soft" sync>` (based on timestamp/sequence numbers)
- :ref:`Hardware syncing <Synchronizing frames externally>` (multi-sensor sub-ms accuracy, hardware trigger)

Software "soft" sync
********************

There are two approaches for software syncing:

- :ref:`Sequece number syncing` - for streams set to the same FPS, sub-ms accuracy can be achieved
- :ref:`Timestamp syncing` - for streams with different FPS, syncing with other sensors either onboard (eg. IMU) or also connected to the host computer (eg. USB ToF sensor)

Sequece number syncing
----------------------

If we want to synchronize multiple messages from the same OAK, such as:

- Camera frames from `ColorCamera <https://docs.luxonis.com/projects/api/en/latest/components/nodes/color_camera/#colorcamera>`__ or `MonoCamera <https://docs.luxonis.com/projects/api/en/latest/components/nodes/mono_camera/#monocamera>`__ (color, left and right frames)
- Messages generated from camera frames (NN results, disparity/depth, edge detections, tracklets, encoded frames, tracked features, etc.)

We can use sequence number syncing, `demos here <https://github.com/luxonis/depthai-experiments/tree/master/gen2-syncing#message-syncing>`__.
Each frame from ColorCamera/MonoCamera will get assigned a sequence number, which then also gets copied to message generated from that frame.

For sequence number syncing **FPS of all cameras need to be the same**. On host or inside script node you can get message's sequence number like this:

.. code-block:: depthai-python

    # Get the message from the queue
    message = queue.get()
    # message can be ImgFrame, NNData, Tracklets, ImgDetections, TrackedFeatures...
    seqNum = message.getSequenceNum()


Through firmware sync, we're monitoring for drift and aligning the capture timestamps of all cameras (left, right, color), which are taken
at the MIPI Start-of-Frame (SoF) event. The Left/Right global shutter cameras are driven by the same clock, started by broadcast write
on I2C, so no drift will happen over time, even when running freely without a hardware sync.

The RGB rolling shutter has a slight difference in clocking/frame-time, so when we detect a small drift, we're modifying the
frame-time (number of lines) for the next frame by a small amount to compensate.

If sensors are set to the same FPS (default is 30), the above two approaches are **already integrated into depthai and enabled**
by default, which allows us to **achieve sub-ms delay between all frames** + messages generated by these frames!

.. code-block:: bash

    [Seq 325] RGB timestamp: 0:02:33.549449
    [Seq 325] Disparity timestamp: 0:02:33.549402
    -----------
    [Seq 326] RGB timestamp: 0:02:33.582756
    [Seq 326] Disparity timestamp: 0:02:33.582715
    -----------
    [Seq 327] RGB timestamp: 0:02:33.616075
    [Seq 327] Disparity timestamp: 0:02:33.616031

Disparity and color frame timestamps indicate that we achieve well below sub-ms accuracy.

Timestamp syncing
-----------------

As opposed to sequence number syncing, **timestamp syncing** can sync:

- **streams** with **different FPS**
- **IMU** results with other messages
- messages with **other devices connected to the computer**, as timestamps are synced to the host computer clock

Feel free to check the `demo here <https://github.com/luxonis/depthai-experiments/tree/master/gen2-syncing#imu--rgb--depth-timestamp-syncing>`__
which uses timestamps to sync IMU, color and disparity frames together, with all of these streams producing messages at different FPS.

In case of **multiple streams having different FPS**, there are 2 options on how to sync them:

#. **Removing some messages** from faster streams to get the synced FPS of the slower stream
#. **Duplicating some messages** from slower streams to get the synced FPS of the fastest stream

Frame timestamp
---------------

The timestamp is assigned to the frame at the MIPI SoF (start of frame) event, when the sensor starts streaming the frame
(MIPI readout).

For global shutter sensors, this follows immediately after the exposure for the whole frame was finished,
so we can say the timestamp assigned is aligned with end of exposure-window (within a margin of few microseconds).
Here's an example graph of the global shutter sensor timings, which demonstrates when timestamp is assigned to the frame:

.. code-block::

    frame-time |---------33.3ms---------|---------33.3ms---------|---------33.3ms---------|-

    exposure           <-----20ms------>                 <-10ms->     <-------30ms------->
              _        _________________                 ________     ____________________
    STROBE     |______|     frame1      |_______________| frame2 |___|      frame3        |_
                _                        _                        _                        _
    FSIN      _| |______________________| |______________________| |______________________|

    MIPI      _     frame0  ____________    frame1   ____________   frame2    ____________
    readout    \XXXXXXXXXXX/            \XXXXXXXXXXX/            \XXXXXXXXXXX/            \

    capture    ^                        ^                        ^                        ^
    timestamp  frame0                   frame1                   frame2                   frame3

For rolling shutter, the example graph looks a bit different.
MIPI SoF follows after the first row of the image was fully exposed and it's being streamed, but the following rows are
still exposing or may have not started exposing yet (depending on exposure time).

Below there's an example graph of rolling shutter sensor (IMX378) at 1080p
and 30fps (33.3ms frame time). MIPI readout time varies between sensors/resolutions, but for IMX378 it's 16.54ms at 1080P,
23.58ms at 4K, and 33.04ms at 12MP.

.. code-block::

    frame-time |---------33.3ms---------|---------33.3ms---------|---------33.3ms---------|-------------...

    exposure           frame1                            frame2       frame3
    exposure  row0     <-----20ms------>                 <-10ms->     <-------30ms------->
    exposure  row540         <-----20ms------>                 <-10ms->     <-------30ms------->
    exposure  row1079              <-----20ms------>                 <-10ms->     <-------30ms------->

    MIPI      _     frame0  ____________    frame1   ____________   frame2    ____________   frame3    _...
    readout    \XXXXXXXXXXX/            \XXXXXXXXXXX/            \XXXXXXXXXXX/            \XXXXXXXXXXX/
                16.54ms                  16.54ms                  16.54ms                  16.54ms
    capture    ^                        ^                        ^                        ^
    timestamp  frame0                   frame1                   frame2                   frame3
                _                        _                        _                        _
    FSYNC     _| |______________________| |______________________| |______________________| |___________...


Synchronizing frames externally
###############################

**Hardware syncing** allows precise synchronization across multiple camera sensors and
potentially with other hardware, e.g. flash LED, external IMU, or other cameras.

This can be done by either **FSIN** or **STROBE** signal (seen above).
Currently, the FSIN I/O is on a different branch `here <https://github.com/luxonis/depthai-python/pull/365>`__ .

For syncing with external LED flash, we suggest using **STROBE**, as you can directly connect it to the LED driver signal. We have done this
on the :ref:`dm9098pro` and :ref:`ng9097pro`, which have on-board illumination IR LED and IR laser dot projector.

OAK-FFC hardware syncing
------------------------

On :ref:`OAK-FFC-4P`, we have 4 camera ports; A (rgb), B (left), C (right), and D (cam_d). A & D are 4-lane MIPI, and B & C
are 2-lane MIPI. Each pair (A&D and B&C) share an I2C bus, and the B&C bus is configured for HW syncing left+right cameras by default.

For A&D ports, you need to explicitly enable hardware syncing:

.. code-block:: python

    cam_A.initialControl.setFrameSyncMode(dai.CameraControl.FrameSyncMode.OUTPUT)
    cam_D.initialControl.setFrameSyncMode(dai.CameraControl.FrameSyncMode.INPUT)

And to get all 4 cameras synced together:

.. code-block:: python

    cam_B.initialControl.setFrameSyncMode(dai.CameraControl.FrameSyncMode.INPUT)
    cam_C.initialControl.setFrameSyncMode(dai.CameraControl.FrameSyncMode.INPUT)

    # AND importantly to tie the FSIN signals of A+D and B+C pairs, by setting a GPIO:
    # OAK-FFC-4P requires driving GPIO6 high (FSIN_MODE_SELECT) to connect together
    # the A+D FSIN group (4-lane pair) with the B+C group (2-lane pair)
    config = dai.Device.Config()
    config.board.gpio[6] = dai.BoardConfig.GPIO(dai.BoardConfig.GPIO.OUTPUT,
                                                dai.BoardConfig.GPIO.Level.HIGH)

    with dai.Device(config) as device:
        device.startPipeline(pipeline)

Additional info can be found in `this forum discussion <https://discuss.luxonis.com/d/934-ffc-4p-hardware-synchronization/3>`__.

Connecting FSIN/STROBE
**********************

All `Series 2 OAK PoE models <https://docs.luxonis.com/projects/hardware/en/latest/pages/articles/oak-s2.html>`__ have an M8 I/O connector 
which exposes FSIN (frame sync) and STROBE (for driving a flash) signals.

If you won't be using the Series 2 OAK PoE model, you will need to solder a wire to the PCB on your device. Here's an example of STROBE trace on the `OAK-D-PoE <https://github.com/luxonis/depthai-hardware/tree/master/SJ2088POE_PoE_Board>`__:

.. image:: https://user-images.githubusercontent.com/18037362/142761081-83742829-2527-4277-ad31-a8da500e1039.png

Connecting a small wire to the test pad TP18 would allow you to externally drive the STROBE signal.

Triggering the FSIN
*******************

External camera triggering of the FSIN will be possible, either by :ref:`RVC2` with `GPIOs in Script node <https://docs.luxonis.com/projects/api/en/latest/components/nodes/script/#interfacing-with-gpios>`__,
or an external source. In this case the exposure starts very shortly after the rising edge on FSIN, so we could have external circuitry driving the STROBE
around the same time. You may need to ensure low-latency for the network communication (e.g no other traffic at the moment), or just enable
the flash at the same time you send the capture trigger command over network, and keep it active for a time of configured camera exposure + some delta.


.. include::  /pages/includes/footer-short.rst
